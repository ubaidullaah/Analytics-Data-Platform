# End-to-End Fintech Data Engineering Project

A comprehensive data engineering pipeline that orchestrates data ingestion, transformation, and analytics for fintech data using Apache Airflow, Snowflake, and DBT.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Setup Instructions](#setup-instructions)
- [Data Flow](#data-flow)
- [Usage](#usage)
- [DAGs](#dags)
- [DBT Models](#dbt-models)
- [Database Schema](#database-schema)
- [Configuration](#configuration)
- [Contributing](#contributing)

## ğŸ¯ Overview

This project implements a complete ETL/ELT pipeline for processing fintech data including:
- User information and KYC data
- Merchant details
- Payment events and transactions
- Chargeback records
- Foreign exchange rates
- Device fingerprinting data

The pipeline follows modern data engineering best practices with:
- **Orchestration**: Apache Airflow for workflow management
- **Data Warehouse**: Snowflake for scalable data storage
- **Transformation**: DBT for SQL-based transformations
- **Containerization**: Docker for easy deployment and development

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Files â”‚
â”‚  (Data Dir) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Airflow â”‚
â”‚   (Orchestrator) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Snowflake  â”‚  â”‚   DBT    â”‚
â”‚   (Storage) â”‚â—„â”€â”¤(Transform)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Analytics â”‚
â”‚    Layer    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

- **Apache Airflow 3.1.5**: Workflow orchestration and scheduling
- **Snowflake**: Cloud data warehouse
- **DBT (Data Build Tool)**: SQL-based data transformation
- **Docker & Docker Compose**: Containerization
- **PostgreSQL**: Airflow metadata database
- **Redis**: Celery message broker
- **Python**: DAG development

## ğŸ“ Project Structure

```
End to End project/
â”œâ”€â”€ Airflow/
â”‚   â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â”‚   â”œâ”€â”€ Snowflake_data_ingestion.py  # Data ingestion DAG
â”‚   â”‚   â””â”€â”€ DBT_transformations.py       # DBT transformation DAG
â”‚   â”œâ”€â”€ config/                        # Airflow configuration
â”‚   â”œâ”€â”€ logs/                          # Airflow execution logs
â”‚   â”œâ”€â”€ plugins/                       # Custom Airflow plugins
â”‚   â”œâ”€â”€ docker-compose.yaml            # Docker Compose configuration
â”‚   â””â”€â”€ Dockerfile                     # Custom Airflow image with DBT
â”‚
â”œâ”€â”€ Snowflake/
â”‚   â””â”€â”€ migrations/                    # Database migration scripts
â”‚       â”œâ”€â”€ V001__database_and_schemas.sql
â”‚       â”œâ”€â”€ V002__file_formats.sql
â”‚       â”œâ”€â”€ V003__stages.sql
â”‚       â”œâ”€â”€ V004__audit_table.sql
â”‚       â””â”€â”€ V005__raw_tables.sql
â”‚
â”œâ”€â”€ DBT_fintech_project/               # DBT transformation project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                   # Staging layer models
â”‚   â”‚   â””â”€â”€ marts/                     # Analytics/marts layer models
â”‚   â”œâ”€â”€ tests/                         # Data quality tests
â”‚   â”œâ”€â”€ macros/                        # DBT macros
â”‚   â””â”€â”€ dbt_project.yml                # DBT project configuration
â”‚
â””â”€â”€ Data Files/                        # Sample CSV data files
    â”œâ”€â”€ raw_users.csv
    â”œâ”€â”€ raw_merchants.csv
    â”œâ”€â”€ raw_payment_events_*.csv
    â”œâ”€â”€ raw_chargebacks_*.csv
    â””â”€â”€ raw_fx_rates_*.csv
```

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:

- **Docker Desktop** (or Docker Engine + Docker Compose)
- **Snowflake Account** with appropriate permissions
- **Python 3.8+** (for local development)
- **DBT CLI** (optional, for local DBT development)
- **Git** (for version control)

## ğŸš€ Setup Instructions

### 1. Clone the Repository

```bash
git clone <repository-url>
cd "End to End project"
```

### 2. Configure Snowflake Connection

Create a `.env` file in the `Airflow/` directory with your Snowflake credentials:

```env
AIRFLOW_UID=50000
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=FINTECH_DW
SNOWFLAKE_SCHEMA=RAW
```

### 3. Configure DBT Profile

Ensure your DBT profile is configured at `~/.dbt/profiles.yml`:

```yaml
fintech_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: your_account
      user: your_username
      password: your_password
      warehouse: your_warehouse
      database: FINTECH_DW
      schema: ANALYTICS
      threads: 4
```

### 4. Set Up Snowflake Database

Run the migration scripts in order:

```bash
# Connect to Snowflake and run migrations
# V001__database_and_schemas.sql
# V002__file_formats.sql
# V003__stages.sql
# V004__audit_table.sql
# V005__raw_tables.sql
```

### 5. Upload Data Files to Snowflake Stage

Upload your CSV files to the Snowflake stage:

```sql
PUT file:///path/to/Data Files/*.csv @FINTECH_DW.RAW.FINTECH_STAGE;
```

### 6. Start Airflow Services

Navigate to the Airflow directory and start the services:

```bash
cd Airflow
docker-compose up -d
```

### 7. Access Airflow UI

Open your browser and navigate to:
```
http://localhost:8080
```

Default credentials:
- Username: `airflow`
- Password: `airflow`

## ğŸ”„ Data Flow

### 1. Data Ingestion (`fintech_raw_ingestion` DAG)

The ingestion DAG runs daily and loads data from CSV files in Snowflake stages into raw tables:

1. **Load Raw Users** â†’ `RAW_USERS` table
2. **Load Raw Merchants** â†’ `RAW_MERCHANTS` table
3. **Load Device Fingerprints** â†’ `RAW_DEVICE_FINGERPRINTS` table
4. **Load Payment Events** â†’ `RAW_PAYMENT_EVENTS` table (with date-based file naming)
5. **Load Chargebacks** â†’ `RAW_CHARGEBACKS` table
6. **Load FX Rates** â†’ `RAW_FX_RATES_DAILY` table

### 2. Data Transformation (`dbt_run` DAG)

The DBT DAG runs after ingestion and performs transformations:

1. **Staging Layer**: Cleans and standardizes raw data
   - `stg_raw_users`
   - `stg_raw_merchants`
   - `stg_raw_payment_events`
   - `stg_raw_chargebacks`
   - `stg_raw_device_fingerprints`

2. **Marts Layer**: Business logic and analytics
   - `mart_payment_kpis_daily`
   - `mart_chargeback_kpis_daily`
   - `mart_risk_signals_daily`

3. **Data Quality Tests**: Validates data integrity

## ğŸ“Š DAGs

### `fintech_raw_ingestion`

- **Schedule**: `@daily`
- **Catchup**: Enabled
- **Description**: Ingests raw CSV data from Snowflake stages into raw tables
- **Tasks**: Sequential loading of all raw data sources

### `dbt_run`

- **Schedule**: `@daily`
- **Catchup**: Disabled
- **Description**: Runs DBT transformations and tests
- **Tasks**:
  - `dbt_run`: Executes DBT models
  - `dbt_test`: Runs data quality tests

## ğŸ—„ï¸ Database Schema

### Raw Layer (`FINTECH_DW.RAW`)

- `RAW_USERS`: User information and KYC data
- `RAW_MERCHANTS`: Merchant details and categories
- `RAW_PAYMENT_EVENTS`: Payment transaction events
- `RAW_CHARGEBACKS`: Chargeback records
- `RAW_FX_RATES_DAILY`: Daily foreign exchange rates
- `RAW_DEVICE_FINGERPRINTS`: Device identification data

### Analytics Layer (`FINTECH_DW.ANALYTICS`)

- Staging models (prefixed with `stg_`)
- Mart models (prefixed with `mart_`)

## âš™ï¸ Configuration

### Airflow Configuration

- **Executor**: CeleryExecutor
- **Database**: PostgreSQL
- **Broker**: Redis
- **Web Server Port**: 8080
- **Logs**: Stored in `Airflow/logs/`

### DBT Configuration

- **Materialization Strategy**:
  - Staging: Incremental tables
  - Marts: Tables
- **Target**: Snowflake
- **Profile**: `fintech_project`

## ğŸ§ª Testing

DBT tests are automatically run after model execution. Test files are located in:
- `DBT_fintech_project/tests/`

Run tests manually:
```bash
cd DBT_fintech_project
dbt test
```

## ğŸ“ Development

### Adding New DAGs

1. Create a new Python file in `Airflow/dags/`
2. Define your DAG following Airflow best practices
3. The DAG will be automatically discovered by Airflow

### Adding New DBT Models

1. Create SQL files in `DBT_fintech_project/models/`
2. Organize by layer (staging, marts, intermediate)
3. Run `dbt run` to execute

### Modifying Database Schema

1. Create new migration files in `Snowflake/migrations/`
2. Follow naming convention: `V###__description.sql`
3. Run migrations in order

## ğŸ” Monitoring

- **Airflow UI**: Monitor DAG runs, task logs, and execution history
- **Snowflake**: Query execution history and warehouse usage
- **DBT**: Check run results in `DBT_fintech_project/target/run_results.json`

## ğŸ› Troubleshooting

### Airflow Issues

- Check logs: `Airflow/logs/`
- Verify Docker containers: `docker-compose ps`
- Restart services: `docker-compose restart`

### Snowflake Connection Issues

- Verify credentials in Airflow connections
- Check network connectivity
- Ensure warehouse is running

### DBT Issues

- Verify DBT profile configuration
- Check model compilation: `dbt compile`
- Review error logs in `DBT_fintech_project/logs/`

## ğŸ“š Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Snowflake Documentation](https://docs.snowflake.com/)
- [DBT Documentation](https://docs.getdbt.com/)
- [Docker Documentation](https://docs.docker.com/)

## ğŸ¤ Contributing

1. Create a feature branch
2. Make your changes
3. Test thoroughly
4. Submit a pull request

## ğŸ“„ License

[Specify your license here]

## ğŸ‘¤ Author

[Your Name/Organization]

---

**Note**: This is a development setup. For production deployments, ensure proper security configurations, secrets management, and resource allocation.

